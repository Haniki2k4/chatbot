import requests
from bs4 import BeautifulSoup
import os
import time
import re
import hashlib
import nltk
from nltk.tokenize import sent_tokenize

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download("punkt")
    nltk.download("punkt_tab")

BASE_URL = "https://benhvienducgiang.com"
START_URL = "https://benhvienducgiang.com"
INTRO_URLS = [
    "https://benhvienducgiang.com/gioi-thieu/102-300.aspx",
    "https://benhvienducgiang.com/gioi-thieu-chung/102-648.aspx",
    "https://benhvienducgiang.com/chuc-nang-va-nhiem-vu/102-650.aspx",
]
OUTPUT_DIR = "duc_giang_txt"
HEADERS = {"User-Agent": "Mozilla/5.0"}

VISITED_URLS = set()
CONTENT_HASHES = set()


def normalize_url(url):
    """XÃ³a query params vÃ  fragments"""
    url = url.split("?")[0].split("#")[0]
    return url.rstrip("/")


def get_content_hash(text):
    """MD5 hash 500 kÃ½ tá»± Ä‘áº§u Ä‘á»ƒ detect duplicate"""
    text_normalized = text.lower().strip()[:500]
    return hashlib.md5(text_normalized.encode()).hexdigest()


def get_page_text(url):
    """Láº¥y text tá»« <p> vÃ  div.content"""
    try:
        r = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(r.text, "html.parser")
        paragraphs = soup.find_all("p")
        divs = soup.find_all("div", class_=["content", "article-content", "post-content"])
        text_parts = []
        text_parts.extend([p.get_text(" ", strip=True) for p in paragraphs])
        text_parts.extend([d.get_text(" ", strip=True) for d in divs])
        return " ".join(text_parts)
    except Exception as e:
        print(f"âŒ Lá»—i khi crawl {url}: {str(e)}")
        return ""


def extract_article_links_from_page(url):
    """TrÃ­ch link bÃ i viáº¿t tá»« trang listing"""
    try:
        r = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(r.text, "html.parser")
        article_links = set()
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()
            if href.startswith("#") or href.startswith("javascript"):
                continue
            if "pk_advertisement" in href:
                continue
            if href.startswith("/"):
                href = BASE_URL + href
            elif not href.startswith("http"):
                continue
            href = normalize_url(href)
            if BASE_URL not in href:
                continue
            if href in VISITED_URLS:
                continue
            if any(skip in href.lower() for skip in ["?", "search", "trang-chu", "404", "login"]):
                continue
            article_links.add(href)
        return article_links
    except Exception as e:
        print(f"âš ï¸  Lá»—i khi láº¥y link bÃ i bÃ¡o tá»« {url}: {str(e)}")
        return set()


def extract_links_from_page(url, keyword_filter=None):
    """TrÃ­ch táº¥t cáº£ link tá»« trang"""
    try:
        r = requests.get(url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(r.text, "html.parser")
        links = set()
        for a in soup.find_all("a", href=True):
            href = a["href"].strip()
            if href.startswith("#") or href.startswith("javascript"):
                continue
            if "pk_advertisement" in href:
                continue
            if href.startswith("/"):
                href = BASE_URL + href
            elif not href.startswith("http"):
                continue
            href = normalize_url(href)
            if keyword_filter and keyword_filter not in href:
                continue
            if href in VISITED_URLS:
                continue
            if BASE_URL in href:
                links.add(href)
        return links
    except Exception as e:
        print(f"âš ï¸  Lá»—i khi láº¥y link tá»« {url}: {str(e)}")
        return set()


def get_intro_links(max_depth=2, save_text=False, file_counter=0):
    """Crawl trang giá»›i thiá»‡u (depth-first), save text náº¿u cáº§n"""
    print("ğŸ” Äang tÃ¬m kiáº¿m cÃ¡c link giá»›i thiá»‡u...")
    all_links = set()
    visited = set()
    queue = list(INTRO_URLS)
    depth = {url: 0 for url in INTRO_URLS}
    saved_count = file_counter
    while queue:
        url = queue.pop(0)
        url = normalize_url(url)
        if url in visited or url in VISITED_URLS:
            continue
        if depth.get(url, 0) >= max_depth:
            continue
        visited.add(url)
        VISITED_URLS.add(url)
        print(f"  ğŸ”— Crawling: {url}")
        text = get_page_text(url)
        if len(text) > 300:
            content_hash = get_content_hash(text)
            if content_hash not in CONTENT_HASHES:
                CONTENT_HASHES.add(content_hash)
                all_links.add(url)
                if save_text:
                    file_path = os.path.join(OUTPUT_DIR, f"duc_giang_{saved_count+1}.txt")
                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(text)
                    print(f"  âœ… ÄÃ£ lÆ°u: {file_path}")
                    saved_count += 1
            else:
                print("  âš ï¸  Ná»™i dung trÃ¹ng láº·p, bá» qua")
        article_links = extract_article_links_from_page(url)
        links = extract_links_from_page(url)
        for link in links:
            if link not in visited and link not in VISITED_URLS and depth.get(url, 0) < max_depth:
                queue.append(link)
                depth[link] = depth.get(url, 0) + 1
    print(f"âœ… TÃ¬m tháº¥y {len(all_links)} links giá»›i thiá»‡u")
    if save_text:
        print(f"ğŸ’¾ ÄÃ£ lÆ°u {saved_count - file_counter} files")
    return list(all_links), article_links, saved_count


def get_internal_links(save_text=False, file_counter=0):
    """Crawl trang chá»§, save homepage text náº¿u cáº§n"""
    print("ğŸ” Äang tÃ¬m kiáº¿m cÃ¡c link tá»« trang chá»§...")
    start = normalize_url(START_URL)
    saved_count = file_counter
    if start not in VISITED_URLS:
        VISITED_URLS.add(start)
        if save_text:
            text = get_page_text(start)
            if len(text) > 300:
                content_hash = get_content_hash(text)
                if content_hash not in CONTENT_HASHES:
                    CONTENT_HASHES.add(content_hash)
                    file_path = os.path.join(OUTPUT_DIR, f"duc_giang_{saved_count+1}.txt")
                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(text)
                    print(f"  âœ… ÄÃ£ lÆ°u trang chá»§: {file_path}")
                    saved_count += 1
    links = extract_links_from_page(start)
    article_links = extract_article_links_from_page(start)
    print(f"âœ… TÃ¬m tháº¥y {len(links)} links tá»« trang chá»§")
    return list(links), article_links, saved_count


def crawl_and_save(min_files=25, target_chunks=300):
    """Crawl vÃ  lÆ°u dá»¯ liá»‡u raw text tá»« website"""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    VISITED_URLS.clear()
    CONTENT_HASHES.clear()

    count = 0
    
    print("\n" + "="*60)
    print("ğŸ“š BÆ¯á»šC 1: Crawl cÃ¡c trang giá»›i thiá»‡u")
    print("="*60)
    intro_links, intro_articles, count = get_intro_links(max_depth=2, save_text=True, file_counter=count)
    
    print("\n" + "="*60)
    print("ğŸ  BÆ¯á»šC 2: Crawl trang chá»§")
    print("="*60)
    site_links, site_articles, count = get_internal_links(save_text=True, file_counter=count)

    all_links = []
    seen = set()
    for link in intro_links + site_links:
        if link not in seen and link not in VISITED_URLS:
            all_links.append(link)
            seen.add(link)

    if count < min_files and all_links:
        print(f"\n" + "="*60)
        print("ğŸ“„ BÆ¯á»šC 3: Crawl cÃ¡c link cÃ²n láº¡i")
        print("="*60)
        print(f"ğŸ“¥ Báº¯t Ä‘áº§u crawl {len(all_links)} links cÃ²n láº¡i...")
        
        for i, link in enumerate(all_links, 1):
            if count >= min_files:
                break
            
            if link in VISITED_URLS:
                continue
                
            print(f"[{i}/{len(all_links)}] Äang crawl: {link}")
            text = get_page_text(link)

            if len(text) < 100:
                print("  âš ï¸  Text quÃ¡ ngáº¯n (<100 kÃ½ tá»±), bá» qua")
                VISITED_URLS.add(link)
                continue

            content_hash = get_content_hash(text)
            if content_hash in CONTENT_HASHES:
                print("  âš ï¸  Ná»™i dung trÃ¹ng láº·p, bá» qua")
                VISITED_URLS.add(link)
                continue
            
            CONTENT_HASHES.add(content_hash)

            file_path = os.path.join(OUTPUT_DIR, f"duc_giang_{count+1}.txt")
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(text)

            print(f"  âœ… ÄÃ£ lÆ°u: {file_path}")
            VISITED_URLS.add(link)
            count += 1

            time.sleep(0.5)

    print(f"\n" + "="*60)
    print(f"ğŸ‰ HoÃ n táº¥t crawling!")
    print("="*60)
    print(f"âœ… Tá»•ng sá»‘ file: {count}")
    print(f"ğŸ“„ Tá»« giá»›i thiá»‡u: {len(intro_links)}")
    print(f"ğŸ  Tá»« trang chá»§: 1")
    print(f"ğŸ”— Links bá»• sung: {len(all_links)}")
    print(f"âœ”ï¸ URLs Ä‘Ã£ visit: {len(VISITED_URLS)}")
    print(f"ğŸ” Content hashes: {len(CONTENT_HASHES)}")
    print("="*60)
    
    return count


def load_texts(folder):
    """Load raw text tá»« folder"""
    texts = []
    for file in sorted(os.listdir(folder)):
        if file.endswith(".txt"):
            with open(os.path.join(folder, file), encoding="utf-8") as f:
                texts.append(f.read())
    return texts


def save_chunks_to_file(chunks, output_file="duc_giang_chunks.txt"):
    """LÆ°u cÃ¡c chunks vÃ o file Ä‘á»ƒ tham kháº£o"""
    with open(output_file, "w", encoding="utf-8") as f:
        for i, chunk in enumerate(chunks, 1):
            f.write(f"Chunk {i}:\n")
            f.write(chunk + "\n\n")  # chunk Ä‘Ã£ lÃ  string
    print(f"\nğŸ’¾ ÄÃ£ lÆ°u chunks vÃ o: {output_file}")


def verify_data():
    """Kiá»ƒm tra sá»‘ lÆ°á»£ng file crawled"""
    if not os.path.exists(OUTPUT_DIR):
        print(f"âŒ ThÆ° má»¥c {OUTPUT_DIR} khÃ´ng tá»“n táº¡i")
        return False
    txt_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.txt')]
    if not txt_files:
        print(f"âŒ KhÃ´ng cÃ³ file txt trong {OUTPUT_DIR}")
        return False
    print(f"\nğŸ“Š Thá»‘ng kÃª dá»¯ liá»‡u:")
    print(f"  - Sá»‘ file: {len(txt_files)}")
    texts = load_texts(OUTPUT_DIR)
    total_chars = sum(len(t) for t in texts)
    print(f"  - Tá»•ng sá»‘ kÃ½ tá»±: {total_chars:,}")
    print(f"  - Trung bÃ¬nh: {total_chars // len(txt_files):,} kÃ½ tá»±/file")
    if len(txt_files) < 10:
        print(f"\nâš ï¸  Cáº¢NH BÃO: Sá»‘ file .txt ({len(txt_files)}) < 10!")
        return False
    print(f"\nâœ… Äáº¡t yÃªu cáº§u: {len(txt_files)} files >= 10 files")
    print(f"\nğŸ’¡ Tiáº¿p theo: Cháº¡y chatbot")
    return True


if __name__ == "__main__":
    import sys
    sys.stdout.reconfigure(encoding='utf-8')
    
    print("="*70)
    print("ğŸ•·ï¸  CRAWLER Bá»†NH VIá»†N Äá»¨C GIANG - V3")
    print("="*70)
    print("ğŸ“‹ Quy trÃ¬nh:")
    print("  1. Crawl trang giá»›i thiá»‡u (depth=2)")
    print("  2. Crawl trang chá»§")
    print("  3. LÆ°u raw text (preprocessing á»Ÿ chatbot_engine)")
    print("\nğŸ”§ TÃ­nh nÄƒng:")
    print("  - Normalize URLs")
    print("  - Detect duplicate content (MD5 hash)")
    print("  - Centralized tracking")
    print("\nğŸ“‹ YÃªu cáº§u:")
    print("  - Tá»‘i thiá»ƒu 10 files")
    print("="*70)
    
    num_files = crawl_and_save(min_files=310, target_chunks=300)
    
    if num_files > 0:
        print("\n" + "="*70)
        is_valid = verify_data()
        print("="*70)
        
        if is_valid:
            print("\nâœ… Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng!")
            print(f"ğŸ“‚ ThÆ° má»¥c: {os.path.abspath(OUTPUT_DIR)}")
            print(f"ğŸ”— URLs crawled: {len(VISITED_URLS)}")
            print(f"ğŸ” Unique content: {len(CONTENT_HASHES)}")
            print("\nğŸ’¡ Tiáº¿p theo: python app.py")
        else:
            print("\nâš ï¸  Dá»¯ liá»‡u chÆ°a Ä‘á»§ yÃªu cáº§u!")
            print("ğŸ’¡ Cháº¡y láº¡i crawler hoáº·c tÄƒng min_files Ä‘á»ƒ crawl thÃªm")
    else:
        print("\nâŒ KhÃ´ng crawl Ä‘Æ°á»£c dá»¯ liá»‡u. Vui lÃ²ng kiá»ƒm tra láº¡i.")