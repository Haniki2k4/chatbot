# -*- coding: utf-8 -*-
"""
Chatbot Engine v·ªõi BERT Embedding, LLM Local v√† m√¥ h√¨nh x√°c su·∫•t
- Ti·ªÅn x·ª≠ l√Ω (preprocessing)
- Chunking (t√°ch nh·ªè documents)
- Embedding v√† semantic search
- LLM Local cho t·∫°o c√¢u tr·∫£ l·ªùi (t√πy ch·ªçn)
"""

import os
import re
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pickle
from pathlib import Path

# Import LLM local (optional)
try:
    from llama_cpp import Llama
    HAS_LLM = True
except ImportError:
    HAS_LLM = False
    print("‚ö†Ô∏è  llama-cpp-python ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Chatbot s·∫Ω ch·ªâ s·ª≠ d·ª•ng BERT embeddings.")


# ==================== PREPROCESSING & CHUNKING ====================

PREPROCESS_VERSION = "v4"

def preprocess_text(text):
    """Chu·∫©n h√≥a: lowercase, x√≥a HTML/URL/email, k√Ω t·ª± ƒë·∫∑c bi·ªát"""
    text = text.lower()
    text = re.sub(r"\n+", " ", text)
    text = re.sub(r"<[^>]+>", " ", text)
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"\S+@\S+", "", text)
    text = re.sub(r"(?<=\d)\.(?=\d)", "", text)
    text = re.sub(r"[^\w\s√Ä-·ªπ]", " ", text)
    return text.strip()


def split_sentences(text):
    """T√°ch th√†nh c√¢u b·∫±ng d·∫•u c√¢u"""
    text = re.sub(r"<[^>]+>", " ", text)
    text = re.sub(r"http\S+", " ", text)
    text = re.sub(r"\S+@\S+", " ", text)
    text = re.sub(r"[\r\n]+", ". ", text)
    text = re.sub(r"\s+", " ", text)
    sentences = [s.strip() for s in re.split(r"[\.\!\?]+", text) if s.strip()]
    return sentences


def chunk_text(text, chunk_size=3):
    """Gh√©p c√¢u th√†nh chunks (m·∫∑c ƒë·ªãnh 3 c√¢u)"""
    sentences = split_sentences(text)
    chunks = []
    for i in range(0, len(sentences), chunk_size):
        chunk = ". ".join(sentences[i:i+chunk_size])
        if chunk:
            chunks.append(chunk)
    return chunks


class DucGiangChatbot:
    def __init__(self, data_folder="duc_giang_txt", model_name="sentence-transformers/all-MiniLM-L6-v2", 
                 llm_model_path="E:/NLP/het-mon/chatbot/models/qwen2.5-1.5b-instruct-q4_k_m.gguf", use_llm=True):
        """
        Kh·ªüi t·∫°o chatbot v·ªõi BERT embeddings v√† LLM local 
        
        Args:
            data_folder: Th∆∞ m·ª•c ch·ª©a c√°c file txt ƒë√£ crawl
            model_name: T√™n model BERT ƒë·ªÉ embedding
            llm_model_path: ƒê∆∞·ªùng d·∫´n t·ªõi model GGUF (VD: "models/llama-3.2-1b-instruct-q4_k_m.gguf")
            use_llm: C√≥ s·ª≠ d·ª•ng LLM ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi kh√¥ng (m·∫∑c ƒë·ªãnh False)
        """
        self.data_folder = data_folder
        self.model_name = model_name
        self.llm_model_path = llm_model_path
        self.use_llm = use_llm
        self.embedder = None
        self.llm = None
        self.chunks = []
        self.chunk_clean = []
        self.chunk_embeddings = None
        self.cache_file = "chatbot_cache.pkl"
        
        print("üîÑ ƒêang kh·ªüi t·∫°o chatbot...")
        self._initialize()
        
        # Lu√¥n kh·ªüi t·∫°o LLM local
        self._initialize_llm()
        
        print("‚úÖ Chatbot ƒë√£ s·∫µn s√†ng!")
    
    def _initialize_llm(self):
        """Kh·ªüi t·∫°o LLM local"""
        if not HAS_LLM:
            raise RuntimeError("llama-cpp-python ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        
        if not self.llm_model_path or not os.path.exists(self.llm_model_path):
            raise RuntimeError(f"Kh√¥ng t√¨m th·∫•y model LLM t·∫°i: {self.llm_model_path}")
        
        try:
            print(f"ü§ñ ƒêang load LLM t·ª´: {self.llm_model_path}")
            self.llm = Llama(
                model_path=self.llm_model_path,
                n_ctx=512,  # Context window
                n_threads=max(1, os.cpu_count() // 2),
                n_batch=64,
                verbose=False
            )
            print("‚úÖ LLM ƒë√£ s·∫µn s√†ng!")
        except Exception as e:
            raise RuntimeError(f"L·ªói khi load LLM: {e}")
    
    def _initialize(self):
        """Kh·ªüi t·∫°o model v√† load d·ªØ li·ªáu"""
        # Load BERT model
        print(f"üì• ƒêang load model: {self.model_name}")
        self.embedder = SentenceTransformer(self.model_name)
        
        # Ki·ªÉm tra cache
        if os.path.exists(self.cache_file):
            print("üìÇ T√¨m th·∫•y cache, ƒëang load...")
            self._load_cache()
        else:
            print("üî® Kh√¥ng t√¨m th·∫•y cache, ƒëang x√¢y d·ª±ng index...")
            self._build_index()
            self._save_cache()
    
    def _load_texts(self):
        """ƒê·ªçc t·∫•t c·∫£ file txt t·ª´ th∆∞ m·ª•c"""
        texts = []
        data_path = Path(self.data_folder)
        
        if not data_path.exists():
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {self.data_folder}")
        
        txt_files = list(data_path.glob("*.txt"))
        if not txt_files:
            raise FileNotFoundError(f"Kh√¥ng c√≥ file txt trong th∆∞ m·ª•c: {self.data_folder}")
        
        for filepath in txt_files:
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()
                if content.strip():
                    texts.append(content)
        
        print(f"üìñ ƒê√£ load {len(texts)} file txt")
        return texts
    
    def _build_index(self):
        """X√¢y d·ª±ng index t·ª´ d·ªØ li·ªáu"""
        # Load v√† preprocess texts
        docs = self._load_texts()
        
        # T·∫°o chunks v·ªõi preprocessing
        all_chunks = []
        all_chunk_clean = []
        total_sentences = 0
        for doc in docs:
            # **Preprocessing + Chunking ƒë∆∞·ª£c th·ª±c hi·ªán ·ªü ƒë√¢y**
            sentences = split_sentences(doc)
            total_sentences += len(sentences)
            chunks = chunk_text(doc, chunk_size=3)
            for ch in chunks:
                all_chunks.append(self._normalize_display(ch))
                all_chunk_clean.append(preprocess_text(ch))
        
        self.chunks = all_chunks
        self.chunk_clean = all_chunk_clean
        print(f"üìù T·ªïng s·ªë c√¢u: {total_sentences}")
        print(f"‚úÇÔ∏è ƒê√£ t·∫°o {len(self.chunks)} chunks")
        
        # T·∫°o embeddings
        print("üßÆ ƒêang t·∫°o embeddings...")
        self.chunk_embeddings = self.embedder.encode(
            self.chunk_clean,
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=True
        )
        print(f"‚úÖ ƒê√£ t·∫°o embeddings: {self.chunk_embeddings.shape}")
    
    def _save_cache(self):
        """L∆∞u cache ƒë·ªÉ tƒÉng t·ªëc l·∫ßn sau"""
        cache_data = {
            "chunks": self.chunks,
            "chunk_clean": self.chunk_clean,
            "embeddings": self.chunk_embeddings,
            "preprocess_version": PREPROCESS_VERSION
        }
        with open(self.cache_file, "wb") as f:
            pickle.dump(cache_data, f)
        print(f"üíæ ƒê√£ l∆∞u cache v√†o {self.cache_file}")
    
    def _load_cache(self):
        """Load cache ƒë√£ l∆∞u"""
        with open(self.cache_file, "rb") as f:
            cache_data = pickle.load(f)
        if cache_data.get("preprocess_version") != PREPROCESS_VERSION:
            print("‚ôªÔ∏è Cache c≈© kh√¥ng c√≤n ph√π h·ª£p, ƒëang x√¢y d·ª±ng l·∫°i index...")
            self._build_index()
            self._save_cache()
            return
        self.chunks = cache_data["chunks"]
        self.chunk_clean = cache_data.get("chunk_clean", [])
        self.chunk_embeddings = cache_data["embeddings"]
        print(f"‚úÖ ƒê√£ load {len(self.chunks)} chunks t·ª´ cache")
    
    def _calculate_threshold(self, query):
        """Ng∆∞·ª°ng similarity c·ªë ƒë·ªãnh"""
        return 0.3
    
    def _calculate_probability_score(self, similarity_scores):
        """
        T√≠nh x√°c su·∫•t d·ª±a tr√™n similarity scores
        S·ª≠ d·ª•ng softmax ƒë·ªÉ chuy·ªÉn scores th√†nh ph√¢n ph·ªëi x√°c su·∫•t
        """
        # Softmax normalization
        exp_scores = np.exp(similarity_scores - np.max(similarity_scores))
        probabilities = exp_scores / exp_scores.sum()
        return probabilities

    def _tokenize_for_match(self, text):
        return re.findall(r"[\w√Ä-·ªπ]+", text.lower())

    def _normalize_display(self, text):
        text = re.sub(r"\s*-\s*", " ", text)
        text = re.sub(r"\s+", " ", text)
        return text.strip()
    
    def _generate_llm_response(self, query, contexts):
        """
        Sinh c√¢u tr·∫£ l·ªùi t·ª´ LLM d·ª±a tr√™n contexts
        
        Args:
            query: C√¢u h·ªèi ng∆∞·ªùi d√πng
            contexts: Danh s√°ch c√°c chunks c√≥ li√™n quan
            
        Returns:
            C√¢u tr·∫£ l·ªùi t·ª´ LLM
        """
        if not self.use_llm or not self.llm:
            return None
        
        # T·∫°o context text
        context_text = "\n- ".join(contexts)
        
        # T·∫°o prompt theo ƒë·ªãnh d·∫°ng Qwen/Llama
        prompt = f"""<|im_start|>system
B·∫°n l√† tr·ª£ l√Ω ·∫£o c·ªßa B·ªánh vi·ªán ƒê·ª©c Giang.
Tr·∫£ l·ªùi ng·∫Øn g·ªçn, t·ª± nhi√™n, ƒë√∫ng tr·ªçng t√¢m.
Ch·ªâ d√πng th√¥ng tin trong ng·ªØ c·∫£nh. N·∫øu kh√¥ng ƒë·ªß th√¥ng tin, n√≥i r√µ l√† ch∆∞a t√¨m th·∫•y.
∆Øu ti√™n ti·∫øng Vi·ªát, tr√°nh suy ƒëo√°n.
<|im_end|>
<|im_start|>user
Th√¥ng tin tham kh·∫£o:
{context_text}

C√¢u h·ªèi: {query}
<|im_end|>
<|im_start|>assistant
"""
        
        try:
            # Generate response
            output = self.llm(
                prompt,
                max_tokens=200,
                temperature=0.3,
                top_p=0.9,
                stop=["<|im_end|>", "\n\n"],
                echo=False
            )
            
            response = output["choices"][0]["text"].strip()
            return response if response else None
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói khi sinh c√¢u tr·∫£ l·ªùi t·ª´ LLM: {e}")
            return None
    
    def get_response(self, user_query, top_k=5, return_scores=False):
        """
        L·∫•y c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi c·ªßa user
        
        Args:
            user_query: C√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng
            top_k: S·ªë l∆∞·ª£ng chunks t·ªët nh·∫•t c·∫ßn l·∫•y
            return_scores: C√≥ tr·∫£ v·ªÅ scores kh√¥ng
        
        Returns:
            C√¢u tr·∫£ l·ªùi ho·∫∑c (c√¢u tr·∫£ l·ªùi, scores, inference_time) 
        """
        import time as time_module
        inference_start_time = time_module.time()
        
        if not user_query.strip():
            return "Vui l√≤ng nh·∫≠p c√¢u h·ªèi."
        
        # **Preprocessing: Chu·∫©n h√≥a c√¢u h·ªèi**
        query_clean = preprocess_text(user_query)
        
        # T·∫°o embedding cho query
        query_embedding = self.embedder.encode(
            query_clean,
            convert_to_numpy=True,
            normalize_embeddings=True
        )
        
        # T√≠nh cosine similarity
        similarities = cosine_similarity(
            [query_embedding],
            self.chunk_embeddings
        )[0]

        # ƒêi·ªÉm overlap theo t·ª´ kh√≥a
        query_tokens = set(self._tokenize_for_match(query_clean))
        overlap_scores = np.zeros_like(similarities)
        if query_tokens:
            for i, ch in enumerate(self.chunk_clean):
                ch_tokens = set(self._tokenize_for_match(ch))
                if ch_tokens:
                    overlap_scores[i] = len(query_tokens & ch_tokens) / max(len(query_tokens), 1)

        combined_scores = 0.85 * similarities + 0.15 * overlap_scores
        
        # T√≠nh ng∆∞·ª°ng ƒë·ªông
        threshold = self._calculate_threshold(query_clean)
        
        # L·ªçc theo ng∆∞·ª°ng
        valid_indices = np.where(combined_scores >= threshold)[0]
        
        if len(valid_indices) == 0:
            inference_time = time_module.time() - inference_start_time
            response = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong c∆° s·ªü d·ªØ li·ªáu. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ th√¥ng tin li√™n quan ƒë·∫øn B·ªánh vi·ªán ƒê·ª©c Giang."
            if return_scores:
                return response, [], inference_time
            return response
        
        # L·∫•y top_k best matches trong c√°c k·∫øt qu·∫£ ƒë·∫°t ng∆∞·ª°ng (c·ªë ƒë·ªãnh top_k=5)
        top_k = 5
        valid_scores = combined_scores[valid_indices]
        if len(valid_scores) > 0:
            top_valid_idx = valid_scores.argsort()[-top_k:][::-1]
            top_indices = valid_indices[top_valid_idx]
        else:
            top_indices = combined_scores.argsort()[-top_k:][::-1]
        
        # T√≠nh x√°c su·∫•t
        top_scores = combined_scores[top_indices]
        probabilities = self._calculate_probability_score(top_scores)
        
        # L·∫•y c√°c chunks t∆∞∆°ng ·ª©ng
        responses = []
        scores_info = []
        
        for idx, (chunk_idx, prob) in enumerate(zip(top_indices, probabilities)):
            if combined_scores[chunk_idx] >= threshold:
                responses.append(self.chunks[chunk_idx])
                scores_info.append({
                    "rank": idx + 1,
                    "similarity": float(combined_scores[chunk_idx]),
                    "probability": float(prob),
                    "text": self.chunks[chunk_idx]
                })
        
        # Lo·∫°i b·ªè tr√πng l·∫∑p
        unique_responses = []
        seen = set()
        for resp in responses:
            resp_lower = resp.lower()
            if resp_lower not in seen:
                unique_responses.append(resp)
                seen.add(resp_lower)
        
        # N·∫øu c√≥ LLM, d√πng LLM ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi
        if self.use_llm and unique_responses:
            llm_response = self._generate_llm_response(user_query, unique_responses[:2])
            
            if llm_response:
                final_response = llm_response
            else:
                # Fallback n·∫øu LLM fail
                best_chunk = unique_responses[0]
                if len(unique_responses) > 1:
                    final_response = best_chunk + " " + unique_responses[1]
                else:
                    final_response = best_chunk
        else:
            # Kh√¥ng d√πng LLM: k·∫øt h·ª£p th√†nh c√¢u tr·∫£ l·ªùi ng·∫Øn g·ªçn
            if unique_responses:
                best_chunk = unique_responses[0]
                extra_chunk = unique_responses[1] if len(unique_responses) > 1 else ""
                combined = best_chunk + (". " + extra_chunk if extra_chunk else "")
                combined = re.sub(r"\s+", " ", combined).strip()
                if not combined.endswith(('.', '!', '?')):
                    combined += "."
                final_response = f"D·ª±a tr√™n th√¥ng tin t√¨m ƒë∆∞·ª£c, {combined}"
            else:
                final_response = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p."
        
        if return_scores:
            inference_time = time_module.time() - inference_start_time
            return final_response, scores_info, inference_time
        
        return final_response
    
    def get_stats(self):
        """L·∫•y th·ªëng k√™ v·ªÅ chatbot"""
        return {
            "total_chunks": len(self.chunks),
            "embedding_dim": self.chunk_embeddings.shape[1] if self.chunk_embeddings is not None else 0,
            "model": self.model_name,
            "llm_enabled": self.use_llm,
            "llm_model": self.llm_model_path if self.use_llm else None
        }


# Test n·∫øu ch·∫°y tr·ª±c ti·∫øp
if __name__ == "__main__":
    import sys
    sys.stdout.reconfigure(encoding='utf-8')
    
    bot = DucGiangChatbot()
    
    print("\n" + "="*50)
    print("CHATBOT B·ªÜnh VI·ªÜN ƒê·ª®C GIANG")
    print("="*50)
    print("G√µ 'exit', 'quit' ho·∫∑c 'bye' ƒë·ªÉ tho√°t\n")
    
    while True:
        user_input = input("B·∫°n: ").strip()
        
        if user_input.lower() in ["exit", "quit", "bye", "tho√°t"]:
            print("Bot: C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng d·ªãch v·ª•. T·∫°m bi·ªát!")
            break
        
        if not user_input:
            continue
        
        response, scores = bot.get_response(user_input, return_scores=True)
        print(f"Bot: {response}")
        
        if scores:
            print("\nüìä Th√¥ng tin chi ti·∫øt:")
            for score_info in scores[:2]:
                print(f"  - ƒê·ªô t∆∞∆°ng ƒë·ªìng: {score_info['similarity']:.3f}")
                print(f"  - X√°c su·∫•t: {score_info['probability']:.3f}")
        print()
